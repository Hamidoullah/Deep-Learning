{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Exercice1 : Création d'un Modèle de Langage avec un Simple RNN et Keras\n",
        "## Objectifs :\n",
        "1. Télécharger un corpus textuel et le préparer pour la modélisation.\n",
        "2. Représenter chaque mot avec un vecteur one-hot.\n",
        "3. Construire un modèle RNN simple pour entraîner un modèle de langage.\n",
        "4. Tester le modèle pour prédire le mot suivant et estimer la probabilité d’une phrase.\n",
        "\n",
        "## Étape 1 : Téléchargement et Prétraitement des Données"
      ],
      "metadata": {
        "id": "ho4tBze4c9cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "WKnIFJeEmZG4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UPYrxDioczVl"
      },
      "outputs": [],
      "source": [
        "# Étape 1 : Chargement et prétraitement des données\n",
        "# Téléchargement d'un corpus texte\n",
        "corpus = \"\"\"Nous créons un modèle de langage basé sur un réseau de neurones récurrents.\n",
        "            Ce modèle peut prédire le mot suivant dans une phrase et donner des probabilités\n",
        "            pour des phrases entières.\"\"\"\n",
        "\n",
        "# Tokenization : transformer le texte en une liste de mots\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {v: k for k, v in word_index.items()}  # Dictionnaire inverse\n",
        "\n",
        "# Convertir le texte en séquences d'index\n",
        "sequences = tokenizer.texts_to_sequences([corpus])[0]\n",
        "vocab_size = len(word_index) + 1  # Ajouter 1 pour le padding éventuel\n",
        "\n",
        "# Création de données d'entraînement (X, y)\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(1, len(sequences)):\n",
        "    X.append(sequences[:i])  # Historique des mots\n",
        "    y.append(sequences[i])   # Mot suivant\n",
        "\n",
        "# Padding des séquences pour avoir des longueurs fixes\n",
        "max_sequence_len = max([len(x) for x in X])\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Convertir les labels (y) en one-hot\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Étape 2 : Création du Modèle RNN"
      ],
      "metadata": {
        "id": "xiZglRDDdmbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 2 : Création du modèle\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 10, input_length=max_sequence_len),  # Embedding pour mapper les mots\n",
        "    SimpleRNN(64, activation='tanh'),  # Couche RNN\n",
        "    Dense(vocab_size, activation='softmax')  # Prédiction du prochain mot\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#  Entraînement du modèle\n",
        "model.fit(X, y, epochs=200, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWlKoQQ6drho",
        "outputId": "8c499b87-d7af-4159-c6e6-e9af61f4fcb7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - 2s/step - accuracy: 0.0000e+00 - loss: 3.3261\n",
            "Epoch 2/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 0.0667 - loss: 3.2870\n",
            "Epoch 3/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.1333 - loss: 3.2495\n",
            "Epoch 4/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.2000 - loss: 3.2096\n",
            "Epoch 5/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.2333 - loss: 3.1655\n",
            "Epoch 6/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.2333 - loss: 3.1172\n",
            "Epoch 7/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.2000 - loss: 3.0668\n",
            "Epoch 8/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.2333 - loss: 3.0172\n",
            "Epoch 9/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.2333 - loss: 2.9626\n",
            "Epoch 10/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.2667 - loss: 2.9075\n",
            "Epoch 11/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.2667 - loss: 2.8628\n",
            "Epoch 12/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.3000 - loss: 2.8154\n",
            "Epoch 13/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 0.2667 - loss: 2.7594\n",
            "Epoch 14/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.3333 - loss: 2.7160\n",
            "Epoch 15/200\n",
            "1/1 - 0s - 22ms/step - accuracy: 0.3333 - loss: 2.6799\n",
            "Epoch 16/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.3333 - loss: 2.6406\n",
            "Epoch 17/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.3333 - loss: 2.5967\n",
            "Epoch 18/200\n",
            "1/1 - 0s - 22ms/step - accuracy: 0.3667 - loss: 2.5529\n",
            "Epoch 19/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.3333 - loss: 2.5082\n",
            "Epoch 20/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.3667 - loss: 2.4648\n",
            "Epoch 21/200\n",
            "1/1 - 0s - 32ms/step - accuracy: 0.4000 - loss: 2.4286\n",
            "Epoch 22/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.4667 - loss: 2.3876\n",
            "Epoch 23/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.5000 - loss: 2.3492\n",
            "Epoch 24/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.5667 - loss: 2.3055\n",
            "Epoch 25/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 0.5333 - loss: 2.2627\n",
            "Epoch 26/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.5333 - loss: 2.2252\n",
            "Epoch 27/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 0.5333 - loss: 2.1956\n",
            "Epoch 28/200\n",
            "1/1 - 0s - 27ms/step - accuracy: 0.5333 - loss: 2.1620\n",
            "Epoch 29/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.5333 - loss: 2.1207\n",
            "Epoch 30/200\n",
            "1/1 - 0s - 27ms/step - accuracy: 0.5333 - loss: 2.0920\n",
            "Epoch 31/200\n",
            "1/1 - 0s - 27ms/step - accuracy: 0.5333 - loss: 2.0650\n",
            "Epoch 32/200\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.5333 - loss: 2.0317\n",
            "Epoch 33/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.6000 - loss: 1.9866\n",
            "Epoch 34/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 0.5333 - loss: 1.9451\n",
            "Epoch 35/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.5667 - loss: 1.9190\n",
            "Epoch 36/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.5667 - loss: 1.8785\n",
            "Epoch 37/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.5333 - loss: 1.8474\n",
            "Epoch 38/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 0.5333 - loss: 1.8246\n",
            "Epoch 39/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.6000 - loss: 1.8226\n",
            "Epoch 40/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.6333 - loss: 1.8422\n",
            "Epoch 41/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.6333 - loss: 1.8305\n",
            "Epoch 42/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.6333 - loss: 1.7638\n",
            "Epoch 43/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.6667 - loss: 1.7111\n",
            "Epoch 44/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.6000 - loss: 1.7200\n",
            "Epoch 45/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.6000 - loss: 1.6732\n",
            "Epoch 46/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.6000 - loss: 1.6767\n",
            "Epoch 47/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 0.6000 - loss: 1.6459\n",
            "Epoch 48/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.7333 - loss: 1.6074\n",
            "Epoch 49/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.6667 - loss: 1.5722\n",
            "Epoch 50/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.7000 - loss: 1.5708\n",
            "Epoch 51/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.7000 - loss: 1.5145\n",
            "Epoch 52/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.6333 - loss: 1.5063\n",
            "Epoch 53/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.6333 - loss: 1.4908\n",
            "Epoch 54/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.7000 - loss: 1.4527\n",
            "Epoch 55/200\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.7000 - loss: 1.4181\n",
            "Epoch 56/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.7667 - loss: 1.4231\n",
            "Epoch 57/200\n",
            "1/1 - 0s - 28ms/step - accuracy: 0.7333 - loss: 1.3772\n",
            "Epoch 58/200\n",
            "1/1 - 0s - 28ms/step - accuracy: 0.7333 - loss: 1.3641\n",
            "Epoch 59/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.7000 - loss: 1.3421\n",
            "Epoch 60/200\n",
            "1/1 - 0s - 29ms/step - accuracy: 0.8000 - loss: 1.3153\n",
            "Epoch 61/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.8333 - loss: 1.3009\n",
            "Epoch 62/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.7667 - loss: 1.2742\n",
            "Epoch 63/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8000 - loss: 1.2623\n",
            "Epoch 64/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8333 - loss: 1.2306\n",
            "Epoch 65/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 0.8333 - loss: 1.2242\n",
            "Epoch 66/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.8000 - loss: 1.1930\n",
            "Epoch 67/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8000 - loss: 1.1844\n",
            "Epoch 68/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8667 - loss: 1.1490\n",
            "Epoch 69/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.8667 - loss: 1.1419\n",
            "Epoch 70/200\n",
            "1/1 - 0s - 22ms/step - accuracy: 0.8667 - loss: 1.1245\n",
            "Epoch 71/200\n",
            "1/1 - 0s - 22ms/step - accuracy: 0.8667 - loss: 1.1101\n",
            "Epoch 72/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8667 - loss: 1.0740\n",
            "Epoch 73/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.8667 - loss: 1.0785\n",
            "Epoch 74/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8667 - loss: 1.0541\n",
            "Epoch 75/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 0.9000 - loss: 1.0500\n",
            "Epoch 76/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8667 - loss: 1.0099\n",
            "Epoch 77/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9000 - loss: 1.0183\n",
            "Epoch 78/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9333 - loss: 0.9775\n",
            "Epoch 79/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9000 - loss: 0.9762\n",
            "Epoch 80/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.8667 - loss: 0.9480\n",
            "Epoch 81/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.9333 - loss: 0.9326\n",
            "Epoch 82/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9333 - loss: 0.9137\n",
            "Epoch 83/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9333 - loss: 0.8978\n",
            "Epoch 84/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9000 - loss: 0.8853\n",
            "Epoch 85/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.9333 - loss: 0.8640\n",
            "Epoch 86/200\n",
            "1/1 - 0s - 31ms/step - accuracy: 0.9333 - loss: 0.8540\n",
            "Epoch 87/200\n",
            "1/1 - 0s - 27ms/step - accuracy: 0.9333 - loss: 0.8343\n",
            "Epoch 88/200\n",
            "1/1 - 0s - 28ms/step - accuracy: 0.9333 - loss: 0.8246\n",
            "Epoch 89/200\n",
            "1/1 - 0s - 29ms/step - accuracy: 0.9000 - loss: 0.8033\n",
            "Epoch 90/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.9333 - loss: 0.7971\n",
            "Epoch 91/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.9333 - loss: 0.7743\n",
            "Epoch 92/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9000 - loss: 0.7667\n",
            "Epoch 93/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9333 - loss: 0.7472\n",
            "Epoch 94/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.9333 - loss: 0.7392\n",
            "Epoch 95/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9667 - loss: 0.7238\n",
            "Epoch 96/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9333 - loss: 0.7100\n",
            "Epoch 97/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9333 - loss: 0.6993\n",
            "Epoch 98/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9333 - loss: 0.6824\n",
            "Epoch 99/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9000 - loss: 0.6772\n",
            "Epoch 100/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9333 - loss: 0.6702\n",
            "Epoch 101/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9000 - loss: 0.6632\n",
            "Epoch 102/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9333 - loss: 0.6522\n",
            "Epoch 103/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9667 - loss: 0.6299\n",
            "Epoch 104/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9000 - loss: 0.6414\n",
            "Epoch 105/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9000 - loss: 0.6233\n",
            "Epoch 106/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9333 - loss: 0.6185\n",
            "Epoch 107/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 0.9333 - loss: 0.5958\n",
            "Epoch 108/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9000 - loss: 0.5899\n",
            "Epoch 109/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9333 - loss: 0.5723\n",
            "Epoch 110/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 0.9000 - loss: 0.5676\n",
            "Epoch 111/200\n",
            "1/1 - 0s - 53ms/step - accuracy: 0.9333 - loss: 0.5499\n",
            "Epoch 112/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9333 - loss: 0.5490\n",
            "Epoch 113/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.9333 - loss: 0.5295\n",
            "Epoch 114/200\n",
            "1/1 - 0s - 31ms/step - accuracy: 0.9333 - loss: 0.5237\n",
            "Epoch 115/200\n",
            "1/1 - 0s - 37ms/step - accuracy: 0.9667 - loss: 0.5122\n",
            "Epoch 116/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.9333 - loss: 0.5046\n",
            "Epoch 117/200\n",
            "1/1 - 0s - 31ms/step - accuracy: 0.9333 - loss: 0.4929\n",
            "Epoch 118/200\n",
            "1/1 - 0s - 28ms/step - accuracy: 0.9667 - loss: 0.4857\n",
            "Epoch 119/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9667 - loss: 0.4765\n",
            "Epoch 120/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 0.9333 - loss: 0.4652\n",
            "Epoch 121/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 0.9333 - loss: 0.4608\n",
            "Epoch 122/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 0.9667 - loss: 0.4485\n",
            "Epoch 123/200\n",
            "1/1 - 0s - 27ms/step - accuracy: 0.9667 - loss: 0.4431\n",
            "Epoch 124/200\n",
            "1/1 - 0s - 29ms/step - accuracy: 0.9333 - loss: 0.4332\n",
            "Epoch 125/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 0.9333 - loss: 0.4277\n",
            "Epoch 126/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 0.9333 - loss: 0.4178\n",
            "Epoch 127/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.9667 - loss: 0.4117\n",
            "Epoch 128/200\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.9667 - loss: 0.4035\n",
            "Epoch 129/200\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.9333 - loss: 0.3970\n",
            "Epoch 130/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.9333 - loss: 0.3898\n",
            "Epoch 131/200\n",
            "1/1 - 0s - 35ms/step - accuracy: 0.9667 - loss: 0.3825\n",
            "Epoch 132/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.9667 - loss: 0.3762\n",
            "Epoch 133/200\n",
            "1/1 - 0s - 33ms/step - accuracy: 0.9333 - loss: 0.3700\n",
            "Epoch 134/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.9667 - loss: 0.3633\n",
            "Epoch 135/200\n",
            "1/1 - 0s - 31ms/step - accuracy: 0.9667 - loss: 0.3574\n",
            "Epoch 136/200\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.9667 - loss: 0.3508\n",
            "Epoch 137/200\n",
            "1/1 - 0s - 44ms/step - accuracy: 0.9333 - loss: 0.3453\n",
            "Epoch 138/200\n",
            "1/1 - 0s - 45ms/step - accuracy: 0.9667 - loss: 0.3396\n",
            "Epoch 139/200\n",
            "1/1 - 0s - 53ms/step - accuracy: 0.9667 - loss: 0.3334\n",
            "Epoch 140/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.9333 - loss: 0.3282\n",
            "Epoch 141/200\n",
            "1/1 - 0s - 33ms/step - accuracy: 0.9667 - loss: 0.3229\n",
            "Epoch 142/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.9667 - loss: 0.3171\n",
            "Epoch 143/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 0.9667 - loss: 0.3123\n",
            "Epoch 144/200\n",
            "1/1 - 0s - 59ms/step - accuracy: 0.9667 - loss: 0.3070\n",
            "Epoch 145/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.9667 - loss: 0.3024\n",
            "Epoch 146/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 1.0000 - loss: 0.2977\n",
            "Epoch 147/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 0.9667 - loss: 0.2929\n",
            "Epoch 148/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 0.9667 - loss: 0.2882\n",
            "Epoch 149/200\n",
            "1/1 - 0s - 61ms/step - accuracy: 0.9667 - loss: 0.2837\n",
            "Epoch 150/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 1.0000 - loss: 0.2796\n",
            "Epoch 151/200\n",
            "1/1 - 0s - 40ms/step - accuracy: 1.0000 - loss: 0.2762\n",
            "Epoch 152/200\n",
            "1/1 - 0s - 61ms/step - accuracy: 1.0000 - loss: 0.2764\n",
            "Epoch 153/200\n",
            "1/1 - 0s - 54ms/step - accuracy: 0.9667 - loss: 0.2841\n",
            "Epoch 154/200\n",
            "1/1 - 0s - 60ms/step - accuracy: 0.9333 - loss: 0.2859\n",
            "Epoch 155/200\n",
            "1/1 - 0s - 53ms/step - accuracy: 0.9667 - loss: 0.2633\n",
            "Epoch 156/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 0.9667 - loss: 0.2700\n",
            "Epoch 157/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 1.0000 - loss: 0.2567\n",
            "Epoch 158/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 1.0000 - loss: 0.2552\n",
            "Epoch 159/200\n",
            "1/1 - 0s - 59ms/step - accuracy: 1.0000 - loss: 0.2515\n",
            "Epoch 160/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 1.0000 - loss: 0.2443\n",
            "Epoch 161/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 1.0000 - loss: 0.2451\n",
            "Epoch 162/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 1.0000 - loss: 0.2385\n",
            "Epoch 163/200\n",
            "1/1 - 0s - 54ms/step - accuracy: 1.0000 - loss: 0.2364\n",
            "Epoch 164/200\n",
            "1/1 - 0s - 67ms/step - accuracy: 1.0000 - loss: 0.2323\n",
            "Epoch 165/200\n",
            "1/1 - 0s - 35ms/step - accuracy: 1.0000 - loss: 0.2291\n",
            "Epoch 166/200\n",
            "1/1 - 0s - 58ms/step - accuracy: 1.0000 - loss: 0.2244\n",
            "Epoch 167/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 1.0000 - loss: 0.2220\n",
            "Epoch 168/200\n",
            "1/1 - 0s - 35ms/step - accuracy: 1.0000 - loss: 0.2182\n",
            "Epoch 169/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 1.0000 - loss: 0.2147\n",
            "Epoch 170/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 1.0000 - loss: 0.2116\n",
            "Epoch 171/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 1.0000 - loss: 0.2088\n",
            "Epoch 172/200\n",
            "1/1 - 0s - 30ms/step - accuracy: 1.0000 - loss: 0.2053\n",
            "Epoch 173/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.2029\n",
            "Epoch 174/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 1.0000 - loss: 0.1996\n",
            "Epoch 175/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1965\n",
            "Epoch 176/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1939\n",
            "Epoch 177/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 1.0000 - loss: 0.1912\n",
            "Epoch 178/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1881\n",
            "Epoch 179/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1855\n",
            "Epoch 180/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1831\n",
            "Epoch 181/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1802\n",
            "Epoch 182/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1780\n",
            "Epoch 183/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1751\n",
            "Epoch 184/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1730\n",
            "Epoch 185/200\n",
            "1/1 - 0s - 26ms/step - accuracy: 1.0000 - loss: 0.1703\n",
            "Epoch 186/200\n",
            "1/1 - 0s - 56ms/step - accuracy: 1.0000 - loss: 0.1682\n",
            "Epoch 187/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1657\n",
            "Epoch 188/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1635\n",
            "Epoch 189/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1612\n",
            "Epoch 190/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1591\n",
            "Epoch 191/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1568\n",
            "Epoch 192/200\n",
            "1/1 - 0s - 55ms/step - accuracy: 1.0000 - loss: 0.1548\n",
            "Epoch 193/200\n",
            "1/1 - 0s - 57ms/step - accuracy: 1.0000 - loss: 0.1526\n",
            "Epoch 194/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1505\n",
            "Epoch 195/200\n",
            "1/1 - 0s - 25ms/step - accuracy: 1.0000 - loss: 0.1485\n",
            "Epoch 196/200\n",
            "1/1 - 0s - 24ms/step - accuracy: 1.0000 - loss: 0.1465\n",
            "Epoch 197/200\n",
            "1/1 - 0s - 23ms/step - accuracy: 1.0000 - loss: 0.1445\n",
            "Epoch 198/200\n",
            "1/1 - 0s - 30ms/step - accuracy: 1.0000 - loss: 0.1425\n",
            "Epoch 199/200\n",
            "1/1 - 0s - 34ms/step - accuracy: 1.0000 - loss: 0.1406\n",
            "Epoch 200/200\n",
            "1/1 - 0s - 54ms/step - accuracy: 1.0000 - loss: 0.1387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bb9987b17b0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Étape 3 : Tester le Modèle\n",
        "### Code : Prédire le Mot Suivant"
      ],
      "metadata": {
        "id": "Y7sj0KLtdv8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, tokenizer, input_text, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Prédit le mot suivant pour une séquence donnée.\n",
        "    \"\"\"\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    input_seq = tf.keras.preprocessing.sequence.pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
        "    predicted = np.argmax(model.predict(input_seq), axis=-1)\n",
        "    return tokenizer.index_word[predicted[0]]\n",
        "\n",
        "# Exemple 1 de prédiction\n",
        "input_text = \"Nous créons un modèle de\"\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
        "print(f\"Phrase initiale : '{input_text}'\")\n",
        "print(f\"Mot prédit : '{next_word}'\")\n",
        "# Exemple 2 de prédiction\n",
        "input_text = \"Nous construisons un modèle de\"\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
        "print(f\"Phrase initiale : '{input_text}'\")\n",
        "print(f\"Mot prédit : '{next_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7l12rPLd0tU",
        "outputId": "605232a0-1ec9-4ef3-8cfa-43e2f8eb2bec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Phrase initiale : 'Nous créons un modèle de'\n",
            "Mot prédit : 'langage'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Phrase initiale : 'Nous construisons un modèle de'\n",
            "Mot prédit : 'de'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code : Calculer la Probabilité d’une Phrase"
      ],
      "metadata": {
        "id": "L8Z7aNsLd_dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sentence_probability(model, tokenizer, sentence, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Calcule la probabilité totale d'une phrase.\n",
        "    \"\"\"\n",
        "    sentence_seq = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    prob = 1.0\n",
        "    for i in range(1, len(sentence_seq)):\n",
        "        input_seq = tf.keras.preprocessing.sequence.pad_sequences([sentence_seq[:i]], maxlen=max_sequence_len, padding='pre')\n",
        "        predicted_probs = model.predict(input_seq)\n",
        "        word_prob = predicted_probs[0][sentence_seq[i]]\n",
        "        prob *= word_prob\n",
        "    return prob\n",
        "\n",
        "# Exemple : Probabilité d'une phrase\n",
        "sentence = \"Nous créons un modèle\"\n",
        "prob = compute_sentence_probability(model, tokenizer, sentence, max_sequence_len)\n",
        "print(f\"Phrase : '{sentence}'\")\n",
        "print(f\"Probabilité : {prob}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kin3j7aeCwF",
        "outputId": "87130e00-2f4c-45ed-bda5-5a9760adbc03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Phrase : 'Nous créons un modèle'\n",
            "Probabilité : 0.11122305726755227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXercice 2: Entraînement et Test d'un Modèle Word2Vec\n",
        "L'objectif de ce TP est de comprendre et d'implémenter un modèle Word2Vec pour apprendre les représentations vectorielles de mots (word embeddings) à partir d'un corpus textuel. Vous utiliserez la bibliothèque gensim pour entraîner un modèle Word2Vec, puis testerez les embeddings générés à travers diverses analyses telles que la recherche des mots similaires et les analogies."
      ],
      "metadata": {
        "id": "XIgfScuQrEwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import abc\n",
        "nltk.download('punkt')\n",
        "import gensim.downloader as api\n",
        "corpus = api.load('text8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxlFzVCirFe0",
        "outputId": "11bdb08a-bb53-44ea-deb0-29d125b723b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= gensim.models.Word2Vec(corpus, workers=4)"
      ],
      "metadata": {
        "id": "jWl5pg7xrHeP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(model.wv.key_to_index.keys())\n",
        "data=model.wv['science']\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCvi7zqNsTJM",
        "outputId": "7d5a8df4-e992-4910-b0c7-6154149614f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.23585504 -0.4868843   0.73778105  0.27229264  0.35432124  0.9800996\n",
            " -1.7709215   0.13693951  2.7570286  -0.9942347   0.5860272  -2.8883467\n",
            " -0.7176219  -1.3383068  -2.660156    2.4224882  -2.4666743  -0.8131905\n",
            " -1.750751   -0.39751908 -1.1187401   1.272431   -0.2520156  -0.79906744\n",
            " -0.5683308   0.5270443   2.2557502  -1.9209225  -0.78695375  0.6719341\n",
            "  0.29028213 -1.9436154   0.8765284   1.2289647   0.5062359  -2.081198\n",
            "  0.43020558  2.220376    0.5565584  -2.7431839  -0.16032003 -3.6848242\n",
            "  1.3996514   0.9480189   0.06678706 -1.1434996   0.6670449   0.25639382\n",
            " -0.37875807 -2.0502992   3.4607916   0.7636802  -2.3933957  -2.0550563\n",
            " -0.42045432  0.520604    1.9334751  -0.876951    1.2579758  -0.47389975\n",
            " -0.39349097 -0.994491    0.5972023  -1.142644    1.3027773   1.9590539\n",
            "  3.6296976  -1.5109603   1.4837697   2.4429612  -3.4985023   1.6454178\n",
            " -0.46803385  0.03718194 -1.7377222  -0.19856717  1.6633523   0.23352091\n",
            "  1.1030923  -0.28877056 -0.604741   -0.51883554 -0.82935673 -0.18807092\n",
            "  0.02185178 -0.23079626 -1.0460916  -0.22855447 -0.8690154  -0.15332443\n",
            "  0.25774983  1.1653516  -2.7076066   1.3366457  -1.3216354   3.6904583\n",
            " -0.9346299  -1.1910086   1.6150167   0.45491645]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3b2hlkksUMB",
        "outputId": "ed448711-0af9-4b87-b060-4f89e5ead3a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71290"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('science')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wwEsIxwsWdl",
        "outputId": "d6030307-da52-48df-a491-9da6776f9ae1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('psychology', 0.6578212976455688),\n",
              " ('anthropology', 0.6454300284385681),\n",
              " ('sociology', 0.6031381487846375),\n",
              " ('sciences', 0.6028887033462524),\n",
              " ('humanities', 0.5975008606910706),\n",
              " ('philosophy', 0.5932278037071228),\n",
              " ('memetics', 0.5889267325401306),\n",
              " ('astronomy', 0.5817707180976868),\n",
              " ('zoology', 0.5812671184539795),\n",
              " ('metaphysics', 0.5717838406562805)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('man')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG6lZxi0suvk",
        "outputId": "3cde13b8-cff2-4a52-f9f2-73ebfc66ac16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.7208858132362366),\n",
              " ('girl', 0.6718136072158813),\n",
              " ('creature', 0.5929964780807495),\n",
              " ('stranger', 0.5894483327865601),\n",
              " ('boy', 0.5893861651420593),\n",
              " ('god', 0.5739547610282898),\n",
              " ('lover', 0.5588944554328918),\n",
              " ('gentleman', 0.556574285030365),\n",
              " ('evil', 0.5535324215888977),\n",
              " ('person', 0.5527227520942688)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUpoLzk8swNj",
        "outputId": "a92dc7e9-f3db-49d1-e6c2-b59741c8af6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('welsh', 0.6402224898338318),\n",
              " ('irish', 0.6375089883804321),\n",
              " ('afrikaans', 0.5895422101020813),\n",
              " ('gaelic', 0.5837380886077881),\n",
              " ('scottish', 0.5816424489021301),\n",
              " ('hindi', 0.5793278217315674),\n",
              " ('icelandic', 0.5655657052993774),\n",
              " ('esperanto', 0.5647801756858826),\n",
              " ('dutch', 0.5641345381736755),\n",
              " ('french', 0.5639718770980835)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('paris')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE8dFSCSszlb",
        "outputId": "bcc30e9a-559f-443e-d2ad-1bffa1e18da3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('venice', 0.7488073110580444),\n",
              " ('vienna', 0.7186405062675476),\n",
              " ('munich', 0.7050175070762634),\n",
              " ('milan', 0.7024926543235779),\n",
              " ('bologna', 0.6906524300575256),\n",
              " ('florence', 0.6811869740486145),\n",
              " ('louvre', 0.6811328530311584),\n",
              " ('leipzig', 0.6747598052024841),\n",
              " ('naples', 0.664282500743866),\n",
              " ('france', 0.6623011827468872)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['king', 'woman'], negative=['man'],topn=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSVruiPVs5nT",
        "outputId": "3ded75bb-2d1c-40b4-e3a2-a29ddaa2ca94"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.6441126465797424)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.doesnt_match(['psychology','mathematics','medicine','orange'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4iJbG6a9s6Rr",
        "outputId": "f4a82fb7-1db5-477b-aacb-a0d215f21e14"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'orange'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercice 3: Création d'un Modèle de Langage avec Intégration de Word2Vec"
      ],
      "metadata": {
        "id": "TIm7iA1Uvl4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Étape 1 : Charger un corpus pré-entraîné pour Word2Vec\n",
        "#corpus = api.load('text8')  # Chargement d'un corpus Word2Vec\n",
        "#model = gensim.models.Word2Vec(corpus, vector_size=100, workers=4)  # Word2Vec\n",
        "\n",
        "# Extraire le vocabulaire et les embeddings de Word2Vec\n",
        "word_vectors = model.wv\n",
        "embedding_dim = word_vectors.vector_size\n",
        "vocab_w2v = word_vectors.key_to_index  # Vocabulaire Word2Vec\n",
        "\n",
        "# Étape 2 : Préparer un corpus texte pour le RNN\n",
        "text_corpus = \"\"\"We create a language model based on a recurrent neural network.\n",
        "                 This model can predict the next word in a sentence\n",
        "                 and give probabilities for entire sentences.\"\"\"\n",
        "sentences = [sentence.split() for sentence in text_corpus.split('\\n')]\n",
        "\n",
        "# Tokenization du texte\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([' '.join(sentence) for sentence in sentences])\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {v: k for k, v in word_index.items()}  # Dictionnaire inverse\n",
        "vocab_size = len(word_index) + 1  # Taille du vocabulaire\n",
        "\n",
        "# Étape 3 : Créer une matrice d'embedding basée sur Word2Vec\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in vocab_w2v:  # Vérifier si le mot existe dans Word2Vec\n",
        "        embedding_matrix[i] = word_vectors[word]\n",
        "\n",
        "# Étape 4 : Convertir le corpus en données d'entraînement\n",
        "sequences = tokenizer.texts_to_sequences([' '.join(sentence) for sentence in sentences])\n",
        "\n",
        "# Construire X et y (historique des mots et mot suivant)\n",
        "X, y = [], []\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        X.append(seq[:i])\n",
        "        y.append(seq[i])\n",
        "\n",
        "# Padding des séquences\n",
        "max_sequence_len = max([len(x) for x in X])\n",
        "X = pad_sequences(X, maxlen=max_sequence_len, padding='pre')\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# Étape 5 : Créer le modèle RNN\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size,\n",
        "              output_dim=embedding_dim,\n",
        "              weights=[embedding_matrix],\n",
        "              input_length=max_sequence_len,\n",
        "              trainable=False),  # Couche embedding initialisée avec Word2Vec\n",
        "    SimpleRNN(128, activation='tanh'),\n",
        "    Dense(vocab_size, activation='softmax')  # Prédiction du mot suivant\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Étape 6 : Entraîner le modèle\n",
        "model.fit(X, y, epochs=10, batch_size=64, verbose=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "WTZiLuVQtRYL",
        "outputId": "947c4d67-666c-44f1-dcae-9230814ed061"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Sequential' object has no attribute 'wv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-9901d84972a2>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Extraire le vocabulaire et les embeddings de Word2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mvocab_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m  \u001b[0;31m# Vocabulaire Word2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'wv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 7 : Prédire le mot suivant\n",
        "def predict_next_word(model, tokenizer, input_text, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Prédit le mot suivant pour une séquence donnée.\n",
        "    \"\"\"\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
        "    predicted_index = np.argmax(model.predict(input_seq), axis=-1)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "\n",
        "# Exemple 1 : Prédiction du mot suivant\n",
        "input_text = \"We create a language\"\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
        "print(f\"Phrase initiale : '{input_text}'\")\n",
        "print(f\"Mot prédit : '{next_word}'\")\n",
        "\n",
        "# Exemple 2 : Prédiction du mot suivant\n",
        "input_text = \"We construct a language\"\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
        "print(f\"Phrase initiale : '{input_text}'\")\n",
        "print(f\"Mot prédit : '{next_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzYkRHl6wIAq",
        "outputId": "2515ed51-06b8-4749-dd14-9478fe6992ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
            "Phrase initiale : 'We create a language'\n",
            "Mot prédit : 'model'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Phrase initiale : 'We construct a language'\n",
            "Mot prédit : 'model'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar('create')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHZRypbCgT5v",
        "outputId": "f87c3d04-50a1-4fb7-f26e-ef5e19beb080"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('introduce', 0.7879522442817688),\n",
              " ('adapt', 0.7436988949775696),\n",
              " ('develop', 0.7314819693565369),\n",
              " ('produce', 0.7211201190948486),\n",
              " ('construct', 0.7186512351036072),\n",
              " ('build', 0.7149459719657898),\n",
              " ('generate', 0.7092704176902771),\n",
              " ('enhance', 0.7090157270431519),\n",
              " ('establish', 0.7062076926231384),\n",
              " ('incorporate', 0.6981716156005859)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 8 : Calculer la probabilité d'une phrase\n",
        "def compute_sentence_probability(model, tokenizer, sentence, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Calcule la probabilité totale d'une phrase.\n",
        "    \"\"\"\n",
        "    sentence_seq = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    prob = 1.0\n",
        "    for i in range(1, len(sentence_seq)):\n",
        "        input_seq = pad_sequences([sentence_seq[:i]], maxlen=max_sequence_len, padding='pre')\n",
        "        predicted_probs = model.predict(input_seq)\n",
        "        word_prob = predicted_probs[0][sentence_seq[i]]\n",
        "        prob *= word_prob\n",
        "    return prob\n",
        "\n",
        "\n",
        "# Exemple : Probabilité d'une phrase\n",
        "sentence = \"We create a language model\"\n",
        "prob = compute_sentence_probability(model, tokenizer, sentence, max_sequence_len)\n",
        "print(f\"Phrase : '{sentence}'\")\n",
        "print(f\"Probabilité : {prob}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lNR24S7wOWe",
        "outputId": "cf940a42-bda1-45a8-b4b4-b743b9b809ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Phrase : 'We create a language model'\n",
            "Probabilité : 0.023096473106229542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 4 :Création d'un Modèle d'Analyse de Sentiment avec LSTM et Word2Vec\n",
        "L'analyse de sentiment est un problème classique en traitement automatique du langage naturel (TALN) où le but est de déterminer si un texte exprime une opinion positive ou négative. Dans ce TP, vous utiliserez un corpus annoté contenant des avis (par exemple, des avis de films) pour entraîner un modèle basé sur une architecture LSTM.\n",
        "\n",
        "Les étapes principales incluent :\n",
        "\n",
        "1. Prétraitement des données textuelles.\n",
        "2. Utilisation de Word2Vec pour générer des vecteurs d'embedding pour les mots du corpus.\n",
        "3. Construction d'une architecture LSTM pour capturer les dépendances séquentielles dans les textes.\n",
        "4. Entraînement et évaluation du modèle sur des données annotées."
      ],
      "metadata": {
        "id": "PxhbDAHZyiyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "T-4JvcMLoqOr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 1 : Charger et entraîner le modèle Word2Vec\n",
        "print(\"Chargement et entraînement de Word2Vec...\")\n",
        "corpus = api.load('text8')  # Charger un corpus de texte pré-entraîné\n",
        "word2vec = gensim.models.Word2Vec(corpus, vector_size=100, workers=4)  # Taille de l'embedding = 100\n",
        "word_vectors = word2vec.wv  # Récupérer les vecteurs de mots\n",
        "vocab_size = len(word_vectors)  # Taille du vocabulaire\n",
        "print(f\"Word2Vec entraîné avec un vocabulaire de {vocab_size} mots.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bNVY8TqorP7",
        "outputId": "8cd6688e-0b00-4e94-b2f7-6c46b88f422e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement et entraînement de Word2Vec...\n",
            "Word2Vec entraîné avec un vocabulaire de 71290 mots.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 2 : Prétraitement des données IMDB\n",
        "print(\"Prétraitement des données IMDB...\")\n",
        "max_features = 10000  # Limite des mots les plus fréquents\n",
        "maxlen = 500  # Longueur des séquences\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Ajouter des zéros pour aligner toutes les séquences à maxlen\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrQ1vaWjotHq",
        "outputId": "27e936fe-3da9-4f3f-f093-35ee90564ae6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prétraitement des données IMDB...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 3 : Créer une matrice d'embedding basée sur Word2Vec\n",
        "print(\"Création de la matrice d'embedding Word2Vec...\")\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, index in imdb.get_word_index().items():\n",
        "    if index < max_features:\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "        except KeyError:\n",
        "            continue  # Si le mot n'existe pas dans Word2Vec, on le laisse à zéro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62IKn885ovnL",
        "outputId": "84dbfec8-c015-443b-b670-5e72e2491003"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Création de la matrice d'embedding Word2Vec...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 4 : Construire le modèle LSTM\n",
        "print(\"Construction du modèle LSTM...\")\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_features,\n",
        "              output_dim=embedding_dim,\n",
        "              weights=[embedding_matrix],\n",
        "              input_length=maxlen,\n",
        "              trainable=False),  # Embedding fixe\n",
        "    LSTM(128, return_sequences=True),  # Couche LSTM avec 128 unités\n",
        "    Dropout(0.3),  # Régularisation\n",
        "    LSTM(64, return_sequences=False),  # Couche LSTM avec 128 unités\n",
        "    Dropout(0.3),  # Régularisation\n",
        "    Dense(1, activation='sigmoid')  # Prédiction binaire (positif/négatif)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfzoRBoVoykL",
        "outputId": "6380d0d5-dc64-4d30-ba20-07989d6a5fbc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construction du modèle LSTM...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 5 : Entraîner le modèle\n",
        "print(\"Entraînement du modèle...\")\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zQf80cao3q6",
        "outputId": "636c272c-2321-43c6-f76d-d2d99f1f6050"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entraînement du modèle...\n",
            "Epoch 1/5\n",
            "157/157 - 11s - 67ms/step - accuracy: 0.6250 - loss: 0.6392 - val_accuracy: 0.7104 - val_loss: 0.5629\n",
            "Epoch 2/5\n",
            "157/157 - 10s - 64ms/step - accuracy: 0.7271 - loss: 0.5443 - val_accuracy: 0.7390 - val_loss: 0.5274\n",
            "Epoch 3/5\n",
            "157/157 - 8s - 52ms/step - accuracy: 0.7767 - loss: 0.4763 - val_accuracy: 0.6502 - val_loss: 0.6443\n",
            "Epoch 4/5\n",
            "157/157 - 11s - 72ms/step - accuracy: 0.7492 - loss: 0.5167 - val_accuracy: 0.7780 - val_loss: 0.4724\n",
            "Epoch 5/5\n",
            "157/157 - 10s - 62ms/step - accuracy: 0.7953 - loss: 0.4467 - val_accuracy: 0.8036 - val_loss: 0.4361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 6 : Évaluer le modèle\n",
        "print(\"Évaluation sur les données de test...\")\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Précision sur les données de test : {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPatxXOJo4h6",
        "outputId": "29b1483e-3990-46f2-ff14-df2996d25c58"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Évaluation sur les données de test...\n",
            "Précision sur les données de test : 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 7 : Tester le modèle avec un exemple\n",
        "print(\"Prédiction sur un exemple...\")\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "\n",
        "def decode_review(sequence):\n",
        "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in sequence])\n",
        "\n",
        "sample_index = 1\n",
        "sample_review = x_test[sample_index]\n",
        "predicted_sentiment = model.predict(sample_review.reshape(1, -1))[0][0]\n",
        "decoded_review = ' '.join(decoded_review.replace('?', '').split())\n",
        "print(f\"Avis : {decoded_review.replace('?', '')}\")\n",
        "print(f\"Sentiment prédit : {'Positif' if predicted_sentiment > 0.5 else 'Négatif'} (score : {predicted_sentiment:.2f})\")\n"
      ],
      "metadata": {
        "id": "kJr876E40vin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9781c4cb-39f2-414f-e76a-0a927628f5ed"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prédiction sur un exemple...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Avis : this film requires a lot of patience because it focuses on mood and character development the plot is very simple and many of the scenes take place on the same set in frances the sandy dennis character apartment but the film builds to a disturbing climax br br the characters create an atmosphere with sexual tension and psychological it's very interesting that robert altman directed this considering the style and structure of his other films still the trademark altman audio style is evident here and there i think what really makes this film work is the brilliant performance by sandy dennis it's definitely one of her darker characters but she plays it so perfectly and convincingly that it's scary michael burns does a good job as the mute young man regular altman player michael murphy has a small part the moody set fits the content of the story very well in short this movie is a powerful study of loneliness sexual and desperation be patient up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n",
            "Sentiment prédit : Positif (score : 0.84)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFeVjdSXpJLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}